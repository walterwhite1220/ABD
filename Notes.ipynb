{"cells":[{"cell_type":"markdown","source":["## Spack Intro\n\n1. Apache Spark is the most actively developed open-source engine for data processing on computer clusters. \n2. This engine is now a standard data processing tool for developers and data scientists who want to work with big data. \n3. Spark supports a variety of common languages (Python, Java, Scala, and R), includes libraries for a variety of tasks, from ETL to streaming to machine learning to graph processing & analytics.\n4. Spark can run on a laptop to a cluster of thousands of servers. Spark can be deployed on Mesos, Hadoop via YARN, or Spark’s own cluster manager can all be used to deploy it.\n\n## What is spark\n\n1. Spark is an open-source, framework-based component that processes a large amount of unstructured, semi-structured, and structured data for analytics.\n2. Apart from Hadoop and map-reduce architectures for big data processing, Apache Spark’s architecture is regarded as an alternative. \n3. The Resilient Distributed Dataset (RDD) and Directed Acyclic Graph (DAG), Spark’s data storage and processing framework, are utilized to store and process data, respectively. \n4. Spark architecture consists of four components, including the spark driver, executors, cluster administrators, and worker nodes. \n5. It uses the Dataset and data frames as the fundamental data storage mechanism to optimize the Spark process and big data computation.\n\n## Why Spark?\n\nSpark is used to apply transformations on Big Data\n\nTwo scenarios in which it is particularly useful:\nWhen data is too large (Big Data)\nWhen we want to accelerate a calculation\n\n\n### The following are main features of Spark\n\n1. Speed\nSpark performs up to 100 times faster than MapReduce for processing large amounts of data. It is also able to divide the data into chunks in a controlled way.\n2. Powerful Caching\nPowerful caching and disk persistence capabilities are offered by a simple programming layer.\n3. Deployment\nMesos, Hadoop via YARN, or Spark’s own cluster manager can all be used to deploy it.\n4. Real-Time\nBecause of its in-memory processing, it offers real-time computation and low latency.\n5. Polyglot\nIn addition to Java, Scala, Python, and R, Spark also supports all four of these languages. You can write Spark code in any one of these languages. Spark also provides a command-line interface in Scala and Python\n\n## Spark v/s Map Reduce\n\nThe following are differences between Spark & Map Reduce:\n1. Spark is faster as it processes data in RAM (memory) while Hadoop reads and writes files to HDFS (on disk)\n2. Spark is optimized for better parallelism , CPU utilization , and faster startup\n3. Spark has richer functional programming model\n4. Spark is especially useful for iterative algorithms\n\n\n## Spark Architecture \n\nA high-level view of the architecture of the Apache Spark application is as follows:\n1. Driver\n2. Cluster Manager\n3. Worker Nodes\n4. Executors\n\n## Driver\n\n1. The Driver Program is a process that runs the main() function of the application and creates the SparkContext object. \n2. The purpose of SparkContext is to coordinate the spark applications, running as independent sets of processes on a cluster.\n3. To run on a cluster, the SparkContext connects to a different type of cluster managers and then perform the following tasks: -\na. It acquires executors on nodes in the cluster.\nb. Then, it sends your application code to the executors. Here, the application code can be defined by JAR or Python files passed to the SparkContext.\nc. At last, the SparkContext sends tasks to the executors to run.\n\n## Cluster Manager\n\n1. The role of the cluster manager is to allocate resources across applications. The Spark is capable enough of running on a large number of clusters.\n2. It consists of various types of cluster managers such as Hadoop YARN, Apache Mesos and Standalone Scheduler.\n3. Here, the Standalone Scheduler is a standalone spark cluster manager that facilitates to install Spark on an empty set of machines.\n\n## Worker Nodes\n\n1. The worker node is a slave node\n2. Its role is to run the application code in the cluster.\n\n## Executer\n\n1. An executor is a process launched for an application on a worker node.\n2. It runs tasks and keeps data in memory or disk storage across them.\n3. It read and write data to the external sources.\n4. Every application contains its executor\n\n## Spark Components\n\nThe Spark project consists of different types of tightly integrated components. At its core, Spark is a computational engine that can schedule, distribute and monitor multiple applications.\n\nThe following are Spark components:\n1. Spark Core\n2. Spark SQL\n3. Spark Streaming\n4. MLlib\n5. GraphX\n\n## Spark Core\n\n1. The Spark Core is the heart of Spark and performs the core functionality.\n2. It holds the components for task scheduling, fault recovery, interacting with storage systems and memory management.\n\n## Spark SQL\n\n1. The Spark SQL is built on the top of Spark Core. It provides support for structured data.\n2. It allows to query the data via SQL (Structured Query Language) as well as the Apache Hive variant of SQL called the HQL (Hive Query Language).\n3. It supports JDBC and ODBC connections that establish a relation between Java objects and existing databases, data warehouses and business intelligence tools.\n4. It also supports various sources of data like Hive tables, Parquet, and JSON.\n\n## Spark Streaming\n\n1. Spark Streaming is a Spark component that supports scalable and fault-tolerant processing of streaming data.\n2. It uses Spark Core's fast scheduling capability to perform streaming analytics.\n3. It accepts data in mini-batches and performs RDD transformations on that data.\n4. Its design ensures that the applications written for streaming data can be reused to analyze batches of historical data with little modification.\n5. The log files generated by web servers can be considered as a real-time example of a data stream.\n\n## Mlib\n\n1. The MLlib is a Machine Learning library that contains various machine learning algorithms.\n2. These include correlations and hypothesis testing, classification and regression, clustering, and principal component analysis.\n3. It is nine times faster than the disk-based implementation used by Apache Mahout.\n\n## GraphX\n\n1. The GraphX is a library that is used to manipulate graphs and perform graph-parallel computations.\n2. It facilitates to create a directed graph with arbitrary properties attached to each vertex and edge.\n3. To manipulate graph, it supports various fundamental operators like subgraph, join Vertices, and aggregate Messages.\n\n## Spark Componets\n\nSpark Jobs / Spark Tasks\n\n1. A Spark Job(s) are composed of tasks\n2. A Spark Task(s) are actual computation or transformation\n\nActions & Transformations\n\n1. Actions & Transformations are output of a Task\n2. If a task returns a DataFrame, Dataset, or RDD, it is a transformation. \n3. If a task  returns anything else or does not return a value at all it is an action.\n\nLazy Evaluation\n\n1. Lazy Evaluation is a trick commonly used for large data processing. \n2. Lazy Evaluation means triggering processing only when a Spark action is run and not a Spark transformation. \n3. This allows Spark to prepare a logical and physical execution plan to perform the action efficiently.\n\nWide and Narrow Transformations\n\nThe Spark transformations are divided into:\n1. Wide Transformations: require data shuffle, are naturally the most expensive.\n2. Narrow Transformations: does not require data shuffle\n\nMaximize Parallelism In Spark\n1. Spark’s efficiency is based on processing several tasks in parallel. \n2. This is why optimizing a Spark job often means reading and processing as much data as possible in parallel. \n3. And to achieve this goal, it is necessary to split a dataset into several partitions.\n\nPartitions\n\n1. Partition is a logical chunk of your DataFrame. \n2. When reading the default size is 128 MB\n3. Methods to modify partition : repartition and coalesce\n4. Coalesce: only to reduce the number of partitions in a DataFrame, no shuffle\n5. Repartition: to either increase or decrease the number of partitions, does a full shuffle\n\nCaching\n1. Spark can execute part of the DAG to store expensive intermediate results for downstream operations.\n2. Like transformations, caching is applied when an action is executed\n3. Caching is appropriate if you use the same DataFrame multiple times (EDA or ML model\n4. Aside from this, you should not cache (performance degrades)\n\nResilient Distributed Datasets (RDD)\n1. It is a key tool for data computation. \n2. It enables you to recheck data in the event of a failure, and it acts as an interface for immutable data. \n3. It helps in recomputing data in case of failures, and it is a data structure. \n4. There are two methods for modifying RDDs: transformations and actions.\n\nDirected Acyclic Graph (DAG)\n\n1. The driver converts the program into a DAG for each job. \n2. The Apache Spark Eco-system includes various components such as the API core, Spark SQL, Streaming and real-time processing, MLIB, and Graph X. \n3. A sequence of connection between nodes is referred to as a driver. \n4. As a result, you can read volumes of data using the Spark shell. \n5.You can also use the Spark context  to  run a job / task or to stop a job / task.\n\n## Modes of execution\n\nYou can choose from three different execution modes. \nThese determine where your app’s resources are physically located when you run your app. \nYou can decide where to store resources locally, in a shared location, or in a dedicated location.\nExecution Modes\n\n2. Cluster Mode\n3. Client Mode\n4. Local Mode\n\n## Cluster Mode\n1. Cluster mode is the most frequent way of running Spark Applications. \n2. In cluster mode, a user delivers a pre-compiled JAR, Python script, or R script to a cluster manager.\n3. Once the cluster manager receives the pre-compiled JAR, Python script, or R script, the driver process is launched on a worker node inside the cluster, in addition to the executor processes.\n4. This means that the cluster manager is in charge of all Spark application-related processes.\n\n## Client Mode\n\n1. In contrast to cluster mode, where the Spark driver remains on the client machine that submitted the application, the Spark driver is removed in client mode \n2. Therefore, it is the responsibility the client machine to maintain the Spark driver process. \n3. These machines, usually referred to as gateway machines or edge nodes, are maintained on the client machine.\n\n## Local Mode\n\n1. Local mode runs the entire Spark Application on a single machine, as opposed to the previous two modes, which parallelized the Spark Application through threads on that machine. \n2. As a result, the local mode uses threads instead of parallelized threads. \n3. This is a common way to experiment with Spark, try out your applications, or experiment iteratively without having to make any changes on Spark’s end.\n4. In practice, it is not recommended to use local mode for running production applications.\n\n## Cluster Manager Types\n\nThere are several cluster managers supported by the system:\n1. Standalone\n2. Apache Mesos\n3. Hadoop YARN\n4. Kubernetes\n\n## Standalone\n\n1. A Spark cluster manager is included with the software package to make setting up a cluster easy. \n2. The Resource Manager and Worker are the only Spark Standalone Cluster components that are independent. \n3. There is only one executor that runs tasks on each worker node in Standalone Cluster mode. \n4. When a client establishes a connection with the Standalone Master, requests resources, and begins the execution process, a Standalone Clustered master starts the execution process.\n5. The client here is the application master, and it wants the resources from the resource manager. We have a Web UI to view all clusters and job statistics in the Cluster Manager.\n\n## Apache Mesos\n\nIt can run Hadoop MapReduce and service apps as well as be a general cluster manager. Apache Mesos contributes to the development and management of application clusters by using dynamic resource sharing and isolation. It enables the deployment and administration of applications in large-scale cluster environments. \n\nThe Mesos framework includes three components:\n1. Mesos Master:A Mesos Master cluster provides fault tolerance (the capability to operate and recover from loss when a failure occurs). Because of the Mesos Master design, a cluster contains many Mesos Masters.\n2. Mesos Slave: A Mesos Slave is an instance that delivers resources to the cluster. When a Mesos Master assigns a task, Mesos Slave does not assign resources.\n3. Mesos Frameworks: Applications can request resources from the cluster so that the application can perform the tasks. Mesos Frameworks allow for this.\n\n## Hadoop Yarn\n\nA key feature of Hadoop 2.0 is the improved resource manager. \n\nThe Hadoop ecosystem relies on YARN to handle resources. It consists of the following two components:\n\n1. Resource Manager: It controls the allocation of system resources on all applications. A Scheduler and an Application Manager are included. Applications receive resources from the Scheduler.\n\n2. Node Manager: Each job or application needs one or more containers, and the Node Manager monitors these containers and their usage. Node Manager consists of an Application Manager and a Container Manager. Each task in the MapReduce framework runs in a container. The Node Manager monitors the containers and resource usage, and this is reported to the Resource Manager. A\n\n## Kubernets\n\nKubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.\n\nIt groups containers that make up an application into logical units for easy management and discovery. Kubernetes builds upon 15 years of experience of running production workloads at Google, combined with best-of-breed ideas and practices from the community.\n\n1. Planet Scale:\nDesigned on the same principles that allow Google to run billions of containers a week, Kubernetes can scale without increasing your operations team.\n2. Never Outgrow:\nWhether testing locally or running a global enterprise, Kubernetes flexibility grows with you to deliver your applications consistently and easily no matter how complex your need is.\n3. Run K8s Anywhere: Kubernetes is open source giving you the freedom to take advantage of on-premises, hybrid, or public cloud infrastructure, letting you effortlessly move workloads to where it matters to you."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b6c35b03-1ff4-4159-8b73-584e31b571ab","inputWidgets":{},"title":""}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e85ecbc1-3cfe-4f26-af2a-644856f460c9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4d4cccb4-eb18-43b8-af80-55fe507f9f54","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7abc40e5-dc41-44ab-a00d-c5a98da0aa78","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Notes","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
